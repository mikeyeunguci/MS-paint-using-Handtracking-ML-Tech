Collaborators:
Michael Yeung, Arnav Amin

A creative approach to redesigning MS paint using handtracking technology. With a prediction every 50 milliseconds, the app can track your hand movements and convert the movements into actual drawings. The project is designed around the Handtrack.js library, a machine learning model that uses convolutional neural networks (CNNs) to detect and track hands gestures. Handtrack.js is trained on large datasets of hand images to learn patterns and features that distinguish hands from other objects or background elements in the video feed. Handtrack.js uses AI algorithms to analyze the video frames and make decisions about the presence, position, and movements of hands within the feed.

Functions:
- Created a functional web app
- The ability to control the web app with basic gestures
- The ability to control the web app with at least two custom gestures
- Following good principles of UI design
- Creating a compelling app and application of gestures
- A readme and demo video which explains how these features were implemented and their design rationale

Is there anything special we need to know in order to run your code?
Have a good web camera and run npm start in the terminal!


